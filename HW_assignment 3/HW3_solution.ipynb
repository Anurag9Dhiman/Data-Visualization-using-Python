{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Distance Geometry, Clustering, and Nearest Neighbors\n",
    "\n",
    "In this assignment, you will explore how different learning methods\n",
    "view the same data through the lens of distance and geometry.\n",
    "\n",
    "You will work with the UCI Wine dataset and build a short visualization\n",
    "story connecting data geometry, prediction, and dimensionality.\n",
    "\n",
    "Instructions:\n",
    "- Run this notebook top to bottom.\n",
    "- Complete all TODO blocks.\n",
    "- All figures must be generated by your own code.\n",
    "- Clearly label axes and include short interpretations in markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import pairwise_distances, accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine(as_frame=True)\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Inspect X and y\n",
    "print(\"Dataset Shape:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(\"\\nFeature names:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts().sort_index())\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Familiarization\n",
    "\n",
    "Before fitting any models, explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Plot 1: Class Balance\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "class_counts = y.value_counts().sort_index()\n",
    "bars = ax.bar(class_counts.index, class_counts.values, color=['#ff7f0e', '#2ca02c', '#1f77b4'])\n",
    "ax.set_xlabel('Wine Cultivar', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_title('Class Distribution in Wine Dataset', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_xticklabels(['Class 0', 'Class 1', 'Class 2'])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Plot 2: Feature Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "features_to_plot = ['alcohol', 'color_intensity', 'proline', 'flavanoids']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    for class_label in [0, 1, 2]:\n",
    "        data = X[y == class_label][feature]\n",
    "        ax.hist(data, bins=20, alpha=0.6, label=f'Class {class_label}')\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(f'Distribution of {feature.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Plot 3: Scatter plot of two features\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "\n",
    "for class_label in [0, 1, 2]:\n",
    "    mask = y == class_label\n",
    "    ax.scatter(X[mask]['alcohol'], X[mask]['color_intensity'], \n",
    "               c=colors[class_label], label=f'Class {class_label}', \n",
    "               alpha=0.7, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Alcohol Content', fontsize=12)\n",
    "ax.set_ylabel('Color Intensity', fontsize=12)\n",
    "ax.set_title('Wine Cultivars: Alcohol vs Color Intensity', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Exploratory Analysis\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Class Balance**: The dataset has a moderate class imbalance. Class 1 has the most samples (71), followed by Class 0 (59) and Class 2 (48). This is relatively balanced compared to many real-world datasets.\n",
    "\n",
    "2. **Feature Distributions**: Different features show varying levels of class separability:\n",
    "   - **Alcohol**: Shows good separation, with Class 2 having notably higher alcohol content\n",
    "   - **Color Intensity**: Classes overlap but show different central tendencies\n",
    "   - **Proline**: Exhibits strong separation, especially for Class 0\n",
    "   - **Flavanoids**: Clear differences between classes, particularly Class 2\n",
    "\n",
    "3. **2D Scatter Plot**: The alcohol vs color intensity plot reveals that classes are reasonably separable in this 2D space, though some overlap exists. This suggests distance-based methods may work well.\n",
    "\n",
    "4. **Scale Differences**: Features have very different scales (e.g., alcohol ranges 11-15 while proline ranges 200-1600), making standardization essential for distance-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Distance based methods are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "print(\"Standardized data statistics:\")\n",
    "print(X_scaled.describe())\n",
    "print(\"\\nMean of each feature (should be ~0):\")\n",
    "print(X_scaled.mean())\n",
    "print(\"\\nStd of each feature (should be ~1):\")\n",
    "print(X_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Geometry Through Clustering\n",
    "\n",
    "In this task, you will study how clustering methods impose geometric structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2 features for visualization\n",
    "features = ['alcohol', 'color_intensity']\n",
    "X2 = X_scaled[features].values\n",
    "\n",
    "# K-means with different K values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "k_values = [2, 3, 5]\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters = km.fit_predict(X2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    # Plot points\n",
    "    scatter = ax.scatter(X2[:, 0], X2[:, 1], c=clusters, \n",
    "                        cmap='viridis', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "    # Plot centroids\n",
    "    ax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], \n",
    "              c='red', marker='X', s=200, edgecolors='black', linewidth=2, \n",
    "              label='Centroids')\n",
    "    \n",
    "    ax.set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "    ax.set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "    ax.set_title(f'K-Means with K={k}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means with different initializations (K=3)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, seed in enumerate([42, 100, 200]):\n",
    "    km = KMeans(n_clusters=3, random_state=seed, n_init=1)\n",
    "    clusters = km.fit_predict(X2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X2[:, 0], X2[:, 1], c=clusters, \n",
    "                        cmap='viridis', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "    ax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], \n",
    "              c='red', marker='X', s=200, edgecolors='black', linewidth=2, \n",
    "              label='Centroids')\n",
    "    \n",
    "    ax.set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "    ax.set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "    ax.set_title(f'K-Means (seed={seed})', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-medoids comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "k_values = [2, 3, 5]\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    kmedoids = KMedoids(n_clusters=k, random_state=42)\n",
    "    clusters = kmedoids.fit_predict(X2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X2[:, 0], X2[:, 1], c=clusters, \n",
    "                        cmap='plasma', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "    # Plot medoids\n",
    "    ax.scatter(kmedoids.cluster_centers_[:, 0], kmedoids.cluster_centers_[:, 1], \n",
    "              c='darkblue', marker='D', s=200, edgecolors='black', linewidth=2, \n",
    "              label='Medoids')\n",
    "    \n",
    "    ax.set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "    ax.set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "    ax.set_title(f'K-Medoids with K={k}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct comparison: K-means vs K-medoids (K=3)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# K-means\n",
    "km = KMeans(n_clusters=3, random_state=42)\n",
    "km_clusters = km.fit_predict(X2)\n",
    "axes[0].scatter(X2[:, 0], X2[:, 1], c=km_clusters, \n",
    "                cmap='viridis', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], \n",
    "                c='red', marker='X', s=200, edgecolors='black', linewidth=2, label='Centroids')\n",
    "axes[0].set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "axes[0].set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "axes[0].set_title('K-Means (K=3)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# K-medoids\n",
    "kmedoids = KMedoids(n_clusters=3, random_state=42)\n",
    "km_clusters = kmedoids.fit_predict(X2)\n",
    "axes[1].scatter(X2[:, 0], X2[:, 1], c=km_clusters, \n",
    "                cmap='viridis', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[1].scatter(kmedoids.cluster_centers_[:, 0], kmedoids.cluster_centers_[:, 1], \n",
    "                c='darkblue', marker='D', s=200, edgecolors='black', linewidth=2, label='Medoids')\n",
    "axes[1].set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "axes[1].set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "axes[1].set_title('K-Medoids (K=3)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Clustering Results\n",
    "\n",
    "**K-Means Observations:**\n",
    "- **Effect of K**: As K increases, clusters become more granular. K=2 creates broad regions, K=3 aligns well with the visual structure, and K=5 subdivides the space further.\n",
    "- **Initialization**: Different random seeds can lead to slightly different cluster assignments, though with multiple initializations (n_init=10), K-means typically converges to similar solutions.\n",
    "- **Centroid positioning**: Centroids lie at the geometric mean of cluster points, which may not correspond to actual data points.\n",
    "\n",
    "**K-Medoids Observations:**\n",
    "- **Representatives**: Unlike K-means, medoids are actual data points, making them more interpretable as \"prototype\" examples.\n",
    "- **Robustness**: K-medoids is less sensitive to outliers since it minimizes absolute distances rather than squared distances.\n",
    "- **Similar structure**: For this dataset, K-means and K-medoids produce similar clustering patterns, suggesting the data doesn't have extreme outliers.\n",
    "\n",
    "**Key Insight**: Both methods partition the space based on distance, but they represent clusters differently—K-means uses theoretical centroids while K-medoids uses actual exemplar points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Prototype Methods for Classification\n",
    "\n",
    "Now treat the wine cultivar as a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with different k values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "k_values = [1, 3, 5, 10, 20, 50]\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X2[:, 0].min() - 0.5, X2[:, 0].max() + 0.5\n",
    "y_min, y_max = X2[:, 1].min() - 0.5, X2[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Train k-NN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X2, y)\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot training points\n",
    "    scatter = ax.scatter(X2[:, 0], X2[:, 1], c=y, \n",
    "                        cmap='viridis', edgecolors='black', s=50, linewidth=0.5)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y, knn.predict(X2))\n",
    "    \n",
    "    ax.set_xlabel('Alcohol (standardized)', fontsize=10)\n",
    "    ax.set_ylabel('Color Intensity (standardized)', fontsize=10)\n",
    "    ax.set_title(f'k-NN (k={k})\\nTraining Accuracy: {acc:.3f}', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype-based classifier using K-means per class\n",
    "def prototype_classifier(X_train, y_train, X_test, n_prototypes_per_class=3):\n",
    "    \"\"\"\n",
    "    Build a prototype-based classifier using K-means for each class.\n",
    "    For each class, find k prototypes, then classify test points based on \n",
    "    nearest prototype.\n",
    "    \"\"\"\n",
    "    prototypes = []\n",
    "    prototype_labels = []\n",
    "    \n",
    "    # Find prototypes for each class\n",
    "    for class_label in np.unique(y_train):\n",
    "        X_class = X_train[y_train == class_label]\n",
    "        if len(X_class) >= n_prototypes_per_class:\n",
    "            km = KMeans(n_clusters=n_prototypes_per_class, random_state=42)\n",
    "            km.fit(X_class)\n",
    "            prototypes.extend(km.cluster_centers_)\n",
    "            prototype_labels.extend([class_label] * n_prototypes_per_class)\n",
    "    \n",
    "    prototypes = np.array(prototypes)\n",
    "    prototype_labels = np.array(prototype_labels)\n",
    "    \n",
    "    # Classify test points\n",
    "    distances = cdist(X_test, prototypes)\n",
    "    nearest_prototype = np.argmin(distances, axis=1)\n",
    "    predictions = prototype_labels[nearest_prototype]\n",
    "    \n",
    "    return predictions, prototypes, prototype_labels\n",
    "\n",
    "# Compare prototype methods with different numbers of prototypes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "n_prototypes_list = [1, 2, 5]\n",
    "\n",
    "for idx, n_proto in enumerate(n_prototypes_list):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get predictions and prototypes\n",
    "    y_pred, prototypes, proto_labels = prototype_classifier(X2, y, X2, n_proto)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    Z_pred, _, _ = prototype_classifier(X2, y, np.c_[xx.ravel(), yy.ravel()], n_proto)\n",
    "    Z_pred = Z_pred.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contourf(xx, yy, Z_pred, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot training points\n",
    "    ax.scatter(X2[:, 0], X2[:, 1], c=y, \n",
    "              cmap='viridis', edgecolors='black', s=50, linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Plot prototypes\n",
    "    for class_label in np.unique(proto_labels):\n",
    "        class_prototypes = prototypes[proto_labels == class_label]\n",
    "        ax.scatter(class_prototypes[:, 0], class_prototypes[:, 1], \n",
    "                  c=f'C{class_label}', marker='*', s=500, \n",
    "                  edgecolors='black', linewidth=2, label=f'Class {class_label} prototypes')\n",
    "    \n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    \n",
    "    ax.set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "    ax.set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "    ax.set_title(f'Prototype Classifier ({n_proto} per class)\\nAccuracy: {acc:.3f}', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare k-NN (local) vs Prototype (global) behavior\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# k-NN with k=5 (local)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X2, y)\n",
    "Z_knn = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "axes[0].contourf(xx, yy, Z_knn, alpha=0.3, cmap='viridis')\n",
    "axes[0].scatter(X2[:, 0], X2[:, 1], c=y, \n",
    "               cmap='viridis', edgecolors='black', s=50, linewidth=0.5)\n",
    "axes[0].set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "axes[0].set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "axes[0].set_title('k-NN (k=5): Local Decision Boundaries', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Prototype classifier (global)\n",
    "y_pred, prototypes, proto_labels = prototype_classifier(X2, y, np.c_[xx.ravel(), yy.ravel()], 2)\n",
    "Z_proto = y_pred.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z_proto, alpha=0.3, cmap='viridis')\n",
    "axes[1].scatter(X2[:, 0], X2[:, 1], c=y, \n",
    "               cmap='viridis', edgecolors='black', s=50, linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Get prototypes for plotting\n",
    "_, prototypes, proto_labels = prototype_classifier(X2, y, X2, 2)\n",
    "for class_label in np.unique(proto_labels):\n",
    "    class_prototypes = prototypes[proto_labels == class_label]\n",
    "    axes[1].scatter(class_prototypes[:, 0], class_prototypes[:, 1], \n",
    "                   c=f'C{class_label}', marker='*', s=500, \n",
    "                   edgecolors='black', linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Alcohol (standardized)', fontsize=11)\n",
    "axes[1].set_ylabel('Color Intensity (standardized)', fontsize=11)\n",
    "axes[1].set_title('Prototype Classifier: Global Voronoi Regions', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Classification Results\n",
    "\n",
    "**k-NN Behavior:**\n",
    "- **Small k (k=1)**: Decision boundaries are highly local and complex, following individual points exactly. This leads to perfect training accuracy (overfitting) but irregular decision regions.\n",
    "- **Medium k (k=5-10)**: Balances local and global structure, creating smoother boundaries while still adapting to local patterns.\n",
    "- **Large k (k=50)**: Boundaries become very smooth and global, potentially underfitting as local variations are ignored.\n",
    "\n",
    "**Prototype-Based Classifier:**\n",
    "- **Few prototypes (1 per class)**: Creates simple Voronoi tessellation with linear boundaries between class prototypes. Very global and simple.\n",
    "- **More prototypes (5 per class)**: Allows for more complex, non-linear decision boundaries while maintaining interpretability.\n",
    "- **Global structure**: Unlike k-NN, prototype methods impose a global geometric structure based on distances to a fixed set of representatives.\n",
    "\n",
    "**Local vs Global Comparison:**\n",
    "- **k-NN is adaptive**: Decision boundaries curve around individual points, capturing local structure.\n",
    "- **Prototypes are global**: Decision regions are Voronoi cells determined by a small set of representative points.\n",
    "- **Trade-off**: k-NN provides flexibility but can be unstable; prototypes are stable but less flexible.\n",
    "\n",
    "**Key Insight**: k-NN makes predictions based on local neighborhoods, while prototype methods create global partitions. The choice depends on whether you believe class structure is better captured locally or globally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: When Distance Becomes Fragile\n",
    "\n",
    "Investigate how distance based intuition changes as dimension increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Distance versus dimension\n",
    "n_features = X_scaled.shape[1]\n",
    "dimensions = list(range(1, n_features + 1))\n",
    "mean_nearest_distances = []\n",
    "std_nearest_distances = []\n",
    "max_min_ratios = []\n",
    "\n",
    "for p in dimensions:\n",
    "    # Use first p features\n",
    "    X_p = X_scaled.iloc[:, :p].values\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    distances = pairwise_distances(X_p)\n",
    "    \n",
    "    # For each point, find distance to nearest neighbor\n",
    "    nearest_distances = []\n",
    "    for i in range(len(distances)):\n",
    "        # Get distances to all other points (exclude self)\n",
    "        other_distances = distances[i][distances[i] > 0]\n",
    "        if len(other_distances) > 0:\n",
    "            nearest_distances.append(np.min(other_distances))\n",
    "    \n",
    "    mean_nearest_distances.append(np.mean(nearest_distances))\n",
    "    std_nearest_distances.append(np.std(nearest_distances))\n",
    "    \n",
    "    # Also compute max/min ratio of nearest neighbor distances\n",
    "    if len(nearest_distances) > 0:\n",
    "        max_min_ratios.append(np.max(nearest_distances) / np.min(nearest_distances))\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Mean nearest neighbor distance vs dimension\n",
    "axes[0].plot(dimensions, mean_nearest_distances, 'o-', linewidth=2, markersize=6)\n",
    "axes[0].fill_between(dimensions, \n",
    "                      np.array(mean_nearest_distances) - np.array(std_nearest_distances),\n",
    "                      np.array(mean_nearest_distances) + np.array(std_nearest_distances),\n",
    "                      alpha=0.3)\n",
    "axes[0].set_xlabel('Number of Dimensions', fontsize=12)\n",
    "axes[0].set_ylabel('Mean Nearest Neighbor Distance', fontsize=12)\n",
    "axes[0].set_title('Distance Growth with Dimensionality', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Max/min ratio (relative concentration)\n",
    "axes[1].plot(dimensions, max_min_ratios, 's-', color='red', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Number of Dimensions', fontsize=12)\n",
    "axes[1].set_ylabel('Max/Min Nearest Distance Ratio', fontsize=12)\n",
    "axes[1].set_title('Distance Concentration Effect', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN performance versus dimension\n",
    "dimensions = list(range(1, n_features + 1))\n",
    "knn_accuracies = {k: [] for k in [1, 3, 5, 10, 20]}\n",
    "\n",
    "for p in dimensions:\n",
    "    X_p = X_scaled.iloc[:, :p].values\n",
    "    \n",
    "    for k in knn_accuracies.keys():\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_p, y)\n",
    "        acc = accuracy_score(y, knn.predict(X_p))\n",
    "        knn_accuracies[k].append(acc)\n",
    "\n",
    "# Plot k-NN accuracy vs dimension\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for k, accuracies in knn_accuracies.items():\n",
    "    ax.plot(dimensions, accuracies, 'o-', label=f'k={k}', linewidth=2, markersize=5)\n",
    "\n",
    "ax.set_xlabel('Number of Dimensions', fontsize=12)\n",
    "ax.set_ylabel('Training Accuracy', fontsize=12)\n",
    "ax.set_title('k-NN Performance vs Dimensionality', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0.5, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of pairwise distances in different dimensions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "dims_to_plot = [1, 2, 3, 5, 8, 13]\n",
    "\n",
    "for idx, p in enumerate(dims_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    X_p = X_scaled.iloc[:, :p].values\n",
    "    distances = pairwise_distances(X_p)\n",
    "    \n",
    "    # Get upper triangle (exclude diagonal)\n",
    "    upper_triangle_indices = np.triu_indices_from(distances, k=1)\n",
    "    all_distances = distances[upper_triangle_indices]\n",
    "    \n",
    "    ax.hist(all_distances, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(np.mean(all_distances), color='red', linestyle='--', \n",
    "              linewidth=2, label=f'Mean={np.mean(all_distances):.2f}')\n",
    "    ax.set_xlabel('Pairwise Distance', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.set_title(f'{p}D: std/mean = {np.std(all_distances)/np.mean(all_distances):.3f}', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: The Curse of Dimensionality\n",
    "\n",
    "**Distance Growth:**\n",
    "- As dimensions increase, the mean nearest neighbor distance grows approximately as √d (square root of dimension).\n",
    "- This makes points increasingly far apart in absolute terms, even though they may be \"close\" in some dimensions.\n",
    "\n",
    "**Distance Concentration:**\n",
    "- The max/min ratio decreases as dimension grows, meaning all pairwise distances become more similar.\n",
    "- In high dimensions, the nearest and farthest neighbors become almost equidistant from any given point.\n",
    "- This is visualized in the histograms: distributions become tighter (lower std/mean ratio) and more concentrated around the mean.\n",
    "\n",
    "**Impact on k-NN:**\n",
    "- **Small k values** (k=1, 3) show high training accuracy initially but can become less reliable as all points seem equally distant.\n",
    "- **Larger k values** (k=10, 20) show more stable behavior across dimensions but may miss local structure.\n",
    "- Beyond ~8-10 dimensions, additional features provide diminishing returns for classification.\n",
    "\n",
    "**Key Insight - The Curse of Dimensionality:**\n",
    "In high-dimensional spaces:\n",
    "1. Distances lose discriminative power (everything is far from everything)\n",
    "2. The concept of \"nearest neighbor\" becomes less meaningful\n",
    "3. Volume concentrates in corners and shells, not near the center\n",
    "4. More data is needed to maintain the same density\n",
    "\n",
    "This fundamentally challenges distance-based methods and motivates dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Visualization Story\n",
    "\n",
    "### Research Question\n",
    "**How do distance-based learning methods understand and partition data, and when do geometric intuitions break down?**\n",
    "\n",
    "### The Journey Through Four Tasks\n",
    "\n",
    "#### 1. **Discovery: The Data Has Structure** (Task 1)\n",
    "The wine dataset revealed three distinct cultivars with measurable differences in chemical properties. Features like alcohol content, color intensity, and proline showed clear separation between classes. The 2D scatter plot demonstrated that even in low dimensions, classes occupy different regions of the feature space—suggesting that distance-based methods should work well.\n",
    "\n",
    "**Key Finding**: The data is naturally separable, making it a good candidate for geometric methods.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Imposing Structure: How Algorithms See Geometry** (Task 2)\n",
    "K-means and K-medoids revealed how clustering algorithms impose geometric structure on data:\n",
    "\n",
    "- **K-means** creates Voronoi cells around theoretical centroids, minimizing squared distances\n",
    "- **K-medoids** uses actual data points as representatives, creating more interpretable partitions\n",
    "- Both methods assume spherical clusters and are sensitive to the choice of K\n",
    "\n",
    "The comparison showed that while both methods produce similar clusterings for this dataset, they represent fundamentally different philosophies: theoretical optima (centroids) versus concrete exemplars (medoids).\n",
    "\n",
    "**Key Finding**: Clustering methods partition space based on distance, but the choice of representative (centroid vs medoid) affects interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Local vs Global: Two Ways to Classify** (Task 3)\n",
    "The classification experiments highlighted a crucial distinction:\n",
    "\n",
    "- **k-NN is adaptive and local**: It creates complex, data-driven boundaries by looking at nearby points. Small k leads to overfitting (following every point), while large k creates overly smooth boundaries.\n",
    "\n",
    "- **Prototype classifiers are global**: They partition the entire space based on a fixed set of representatives. This creates simple, interpretable Voronoi regions but may miss local variations.\n",
    "\n",
    "The side-by-side comparison revealed that k-NN boundaries curve around individual points (local adaptation), while prototype boundaries are straight lines between representative points (global tessellation).\n",
    "\n",
    "**Key Finding**: There's a fundamental trade-off between local flexibility (k-NN) and global stability (prototypes).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **When Geometry Fails: The Curse of Dimensionality** (Task 4)\n",
    "The final experiments revealed a profound limitation of distance-based methods:\n",
    "\n",
    "- **Distance concentration**: As dimensions increase, all points become approximately equidistant from each other. The ratio of farthest to nearest neighbor approaches 1.\n",
    "\n",
    "- **Loss of contrast**: In high dimensions, the histogram of pairwise distances becomes increasingly narrow (low std/mean ratio), meaning distances lose their discriminative power.\n",
    "\n",
    "- **Practical impact**: k-NN performance plateaus or even degrades beyond 8-10 dimensions, as the notion of \"nearest\" becomes meaningless.\n",
    "\n",
    "This phenomenon—the curse of dimensionality—explains why raw distance-based methods struggle in high-dimensional spaces and motivates techniques like dimensionality reduction, feature selection, and learned distance metrics.\n",
    "\n",
    "**Key Finding**: In high dimensions, geometric intuition breaks down. Distance becomes a poor measure of similarity, and all points appear equally (dis)similar.\n",
    "\n",
    "---\n",
    "\n",
    "### Evolution of Understanding\n",
    "\n",
    "My understanding evolved from:\n",
    "1. **Optimism** (Task 1): \"The data has clear structure; distance methods should work!\"\n",
    "2. **Appreciation** (Tasks 2-3): \"Different distance-based methods offer different geometric perspectives—local vs global, theoretical vs concrete.\"\n",
    "3. **Caution** (Task 4): \"But all distance-based methods share a fundamental vulnerability: they fail when dimensions grow too large.\"\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Distance-based methods are powerful and interpretable when:\n",
    "- Dimensionality is low (≤10 features)\n",
    "- Features are appropriately scaled\n",
    "- The geometric structure aligns with the problem\n",
    "\n",
    "But they require careful handling in high dimensions, where:\n",
    "- Distances concentrate and lose meaning\n",
    "- More sophisticated approaches (kernel methods, manifold learning, deep learning) may be needed\n",
    "- Feature engineering and dimensionality reduction become critical\n",
    "\n",
    "The geometry of distance is both elegant and fragile—it works beautifully in the spaces we can visualize but becomes treacherous in the high-dimensional spaces where modern machine learning operates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
